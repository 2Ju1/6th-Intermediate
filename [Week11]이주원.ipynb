{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsDTIr_KtSjl",
        "outputId": "300f6513-9f57-40ae-dbbf-280f2f8c2519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-VaG6tzsSIY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "train=pd.read_csv('/content/drive/MyDrive/Euron/train.csv')\n",
        "test=pd.read_csv('/content/drive/MyDrive/Euron/test.csv')\n",
        "submission=pd.read_csv('/content/drive/MyDrive/Euron/gender_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(5, 128),\n",
        "            ## Batch Normalization between 'Layer' and 'Activation function'\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            ## Drop out after 'Activation function'\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "yb4_5q5suPfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set = pd.concat((train.drop(['Survived'], axis = 1), test), axis = 0)\n",
        "\n",
        "data_set = data_set.drop(['PassengerId', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis = 1)\n",
        "data_set = data_set.fillna(data_set.mean())\n",
        "\n",
        "n_train = train.shape[0]\n",
        "train_x, test_x = data_set[:n_train], data_set[n_train:]\n",
        "train_y = train['Survived']\n",
        "\n",
        "train_x = train_x[train_x.keys()].values\n",
        "test_x = test_x[test_x.keys()].values\n",
        "train_y = train_y.values\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "simple_nn = SimpleNN()\n",
        "optimizer = optim.Adam(simple_nn.parameters(), lr=0.01)\n",
        "error = nn.BCELoss()\n",
        "\n",
        "batch_size = 99\n",
        "batch_count = int(len(train_x) / batch_size)\n",
        "\n",
        "for epoch in range(300):\n",
        "    train_loss = 0\n",
        "    num_right = 0\n",
        "    for i in range(batch_count):\n",
        "        start = i * batch_size\n",
        "        end = start + batch_size\n",
        "        tensor_x = torch.FloatTensor(train_x[start:end])\n",
        "        tensor_y = torch.FloatTensor(train_y[start:end]).reshape(-1, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = simple_nn(tensor_x)\n",
        "        loss = error(output, tensor_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * batch_size\n",
        "        result = [1 if out >= 0.5 else 0 for out in output]\n",
        "        num_right += np.sum(np.array(result) == train_y[start:end])\n",
        "\n",
        "    train_loss = train_loss / len(train_x)\n",
        "    accuracy = num_right / len(train_x)\n",
        "\n",
        "    if epoch % 25 == 0:\n",
        "        print('Loss: {} Accuracy: {}% Epoch:{}'.format(train_loss, accuracy, epoch))\n",
        "\n",
        "print('Training Ended')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY41sJeyvP9T",
        "outputId": "daa3c448-ae91-4e3f-a6f1-062fc49859a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6487502985530429 Accuracy: 0.6509539842873177% Epoch:0\n",
            "Loss: 0.5166419612036811 Accuracy: 0.7542087542087542% Epoch:25\n",
            "Loss: 0.4655126631259918 Accuracy: 0.7833894500561167% Epoch:50\n",
            "Loss: 0.4168385366598765 Accuracy: 0.8080808080808081% Epoch:75\n",
            "Loss: 0.40057211452060276 Accuracy: 0.8260381593714927% Epoch:100\n",
            "Loss: 0.3409700459904141 Accuracy: 0.8529741863075196% Epoch:125\n",
            "Loss: 0.3282558454407586 Accuracy: 0.8630751964085297% Epoch:150\n",
            "Loss: 0.29826966093646157 Accuracy: 0.8608305274971941% Epoch:175\n",
            "Loss: 0.27613266971376205 Accuracy: 0.877665544332211% Epoch:200\n",
            "Loss: 0.2533235516813066 Accuracy: 0.8866442199775533% Epoch:225\n",
            "Loss: 0.2562143752972285 Accuracy: 0.9001122334455668% Epoch:250\n",
            "Loss: 0.2377128220266766 Accuracy: 0.8967452300785634% Epoch:275\n",
            "Training Ended\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_test_x=torch.FloatTensor(test_x)\n",
        "with torch.no_grad():\n",
        "  test_output=simple_nn(tensor_test_x)\n",
        "  result=np.array([1 if out >=0.5 else 0 for out in test_output])\n",
        "  submission=pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived':result})\n",
        "  submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "qZnyhQbRz74O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch_Normalization"
      ],
      "metadata": {
        "id": "nkaIOt5G0vzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "SIaMNgew26Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "#number of subprocesses to use for data loading\n",
        "num_workers=0\n",
        "#how many samples per batch to load\n",
        "batch_size=64\n",
        "\n",
        "#convert data to torch.FloatTensor\n",
        "transform=transforms.ToTensor()\n",
        "\n",
        "#get the training and test datasets\n",
        "train_data=datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "test_data=datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
        "\n",
        "#prepare data loaders\n",
        "train_loader=torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
        "test_loader=torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u5HyCVl0vVr",
        "outputId": "090f79b8-1c5c-41eb-e8c4-8395edb4dbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 15353925.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 519044.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3749970.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 6985892.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy()\n",
        "\n",
        "# get one image from the batch\n",
        "img = np.squeeze(images[0])\n",
        "\n",
        "fig = plt.figure(figsize = (3,3))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "WHkCmspv1vUm",
        "outputId": "e30407f1-b3e7-46a4-b633-5626394365bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7adffa920e50>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, use_batch_norm, input_size=784, hidden_dim=256, output_size=10):\n",
        "        super(NeuralNet, self).__init__() # init super\n",
        "\n",
        "        # Default layer sizes\n",
        "        self.input_size = input_size # (28*28 images)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_size = output_size # (number of classes)\n",
        "        # Keep track of whether or not this network uses batch normalization.\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "\n",
        "        # define hidden linear layers, with optional batch norm on their outputs\n",
        "        # layers with batch_norm applied have no bias term\n",
        "        if use_batch_norm:\n",
        "            self.fc1 = nn.Linear(input_size, hidden_dim*2, bias=False)\n",
        "            self.batch_norm1 = nn.BatchNorm1d(hidden_dim*2)\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(input_size, hidden_dim*2)\n",
        "\n",
        "        # define *second* hidden linear layers, with optional batch norm on their outputs\n",
        "        if use_batch_norm:\n",
        "            self.fc2 = nn.Linear(hidden_dim*2, hidden_dim, bias=False)\n",
        "            self.batch_norm2 = nn.BatchNorm1d(hidden_dim)\n",
        "        else:\n",
        "            self.fc2 = nn.Linear(hidden_dim*2, hidden_dim)\n",
        "\n",
        "        # third and final, fully-connected layer\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # flatten image\n",
        "        x = x.view(-1, 28*28)\n",
        "        # all hidden layers + optional batch norm + relu activation\n",
        "        x = self.fc1(x)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.batch_norm1(x)\n",
        "        x = F.relu(x)\n",
        "        # second layer\n",
        "        x = self.fc2(x)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.batch_norm2(x)\n",
        "        x = F.relu(x)\n",
        "        # third layer, no batch norm or activation\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "sYRiubLj3opC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_batchnorm = NeuralNet(use_batch_norm=True)\n",
        "net_no_norm = NeuralNet(use_batch_norm=False)\n",
        "\n",
        "print(net_batchnorm)\n",
        "print()\n",
        "print(net_no_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M_7y4ICYuiV",
        "outputId": "3452bd7e-9d1a-4437-b3b2-57d99157463a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNet(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=False)\n",
            "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=False)\n",
            "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "\n",
            "NeuralNet(\n",
            "  (fc1): Linear(in_features=512, out_features=256, bias=False)\n",
            "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, n_epochs=10):\n",
        "  #number of epochs to train the model\n",
        "  n_epochs=n_epochs\n",
        "\n",
        "  #track losses\n",
        "  losses=[]\n",
        "\n",
        "  #optimization strategy\n",
        "  #specify loss function  (categorical cross-entropy)\n",
        "  criterion=nn.CrossEntropyLoss()\n",
        "\n",
        "  #specify optimizer (stochastic gradient descent) and learning rate=0.01\n",
        "  optimizer=torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "  #set the model to training mode\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "        # monitor training loss\n",
        "        train_loss = 0.0\n",
        "\n",
        "        # train the model\n",
        "\n",
        "        batch_count = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the loss\n",
        "            loss = criterion(output, target)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            # update average training loss\n",
        "            train_loss += loss.item() # add up avg batch loss\n",
        "            batch_count +=1\n",
        "\n",
        "        # print training statistics\n",
        "        losses.append(train_loss/batch_count)\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch,train_loss/batch_count))\n",
        "\n",
        "  # return all recorded batch losses\n",
        "  return losses"
      ],
      "metadata": {
        "id": "w03aLOaN5JTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_batchnorm = train(net_batchnorm)\n",
        "losses_no_norm = train(net_no_norm)\n",
        "# compare\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "#losses_batchnorm = np.array(losses_batchnorm)\n",
        "#losses_no_norm = np.array(losses_no_norm)\n",
        "plt.plot(losses_batchnorm, label='Using batchnorm', alpha=0.5)\n",
        "plt.plot(losses_no_norm, label='No norm', alpha=0.5)\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "wSPEaULCKKat",
        "outputId": "3d145493-320b-4910-c2b4-b12f91355137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Module [NeuralNet] is missing the required \"forward\" function",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-8e65bf088ac1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses_batchnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_batchnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlosses_no_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_no_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# compare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#losses_batchnorm = np.array(losses_batchnorm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-0730f05e885e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, n_epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# forward pass: compute predicted outputs by passing inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;31m# calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Module [{type(self).__name__}] is missing the required \\\"forward\\\" function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Module [NeuralNet] is missing the required \"forward\" function"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, train):\n",
        "    # initialize vars to monitor test loss and accuracy\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "    test_loss = 0.0\n",
        "\n",
        "    # set model to train or evaluation mode\n",
        "    # just to see the difference in behavior\n",
        "    if(train==True):\n",
        "        model.train()\n",
        "    if(train==False):\n",
        "        model.eval()\n",
        "\n",
        "    # loss criterion\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        batch_size = data.size(0)\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average test loss\n",
        "        test_loss += loss.item()*batch_size\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare predictions to true label\n",
        "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(batch_size):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss/len(test_loader.dataset)))\n",
        "\n",
        "    for i in range(10):\n",
        "        if class_total[i] > 0:\n",
        "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "                str(i), 100 * class_correct[i] / class_total[i],\n",
        "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "        else:\n",
        "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "        100. * np.sum(class_correct) / np.sum(class_total),\n",
        "        np.sum(class_correct), np.sum(class_total)))"
      ],
      "metadata": {
        "id": "zymuS10EKPuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test batchnorm case, in *train* mode\n",
        "test(net_batchnorm, train=True)\n",
        "# test batchnorm case, in *evaluation* mode\n",
        "test(net_batchnorm, train=False)\n",
        "# for posterity, test no norm case in eval mode\n",
        "test(net_no_norm, train=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "F7ijf_jsZgDo",
        "outputId": "ccdd45bc-0037-497c-89cb-e3be420d75ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Module [NeuralNet] is missing the required \"forward\" function",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-c064cdb7b439>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test batchnorm case, in *train* mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_batchnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# test batchnorm case, in *evaluation* mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_batchnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for posterity, test no norm case in eval mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-5d6d1fe8588c>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, train)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# forward pass: compute predicted outputs by passing inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Module [{type(self).__name__}] is missing the required \\\"forward\\\" function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Module [NeuralNet] is missing the required \"forward\" function"
          ]
        }
      ]
    }
  ]
}